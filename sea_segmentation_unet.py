import os
import glob
import json
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from PIL import Image  # ê¼­ ìƒë‹¨ì— ì¶”ê°€í•´ ì£¼ì„¸ìš”

# ğŸ“Œ ê²½ë¡œ ì„¤ì •
IMAGE_DIR = r"C:\Users\LEEJINSE\Desktop\sim\ì¶˜ê³„í•™ìˆ ëŒ€íšŒ_ë…¼ë¬¸ì¤€ë¹„\New_sample\ì›ì²œë°ì´í„°\ë™í•´_ì œì£¼í•­2êµ¬ì—­_SEG20\20201221"
JSON_DIR = r"C:\Users\LEEJINSE\Desktop\sim\ì¶˜ê³„í•™ìˆ ëŒ€íšŒ_ë…¼ë¬¸ì¤€ë¹„\New_sample\ë¼ë²¨ë§ë°ì´í„°\ë™í•´_ì œì£¼í•­2êµ¬ì—­_SEG\20201221"
OUTPUT_DIR = r"C:\Users\LEEJINSE\Desktop\sim\ì¶˜ê³„í•™ìˆ ëŒ€íšŒ_ë…¼ë¬¸ì¤€ë¹„\ê²°ê³¼"

# ì¶œë ¥ í´ë” ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ğŸ“Œ í•™ìŠµ ì„¤ì •
IMG_SIZE = (256, 256)
BATCH_SIZE = 8
NUM_CLASSES = 6
EPOCHS = 20

# ğŸ“Œ í´ë˜ìŠ¤ ë§¤í•‘ (ë¼ë²¨ â†’ ì¸ë±ìŠ¤)
CLASS_MAPPING = {
    "sky": 0,
    "sea": 1,
    "island": 2,
    "rock": 3,
    "wharf": 4,
    "others": 5
}

# ğŸ”¹ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_data(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    image_name = os.path.basename(data["imagePath"])
    image_path = os.path.join(IMAGE_DIR, image_name)

    if not os.path.exists(image_path):
        print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
        return None, None

    try:
        image = Image.open(image_path)
        image = image.convert("RGB")
        image = image.resize(IMG_SIZE)
        image = np.array(image)
    except Exception as e:
        print(f"âš ï¸ PIL ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path} | {e}")
        return None, None

    # ğŸ¯ ë‹¨ì¼ ì±„ë„ ì •ìˆ˜ ë§ˆìŠ¤í¬ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ìš©)
    mask = np.zeros(IMG_SIZE, dtype=np.uint8)

    for shape in data["shapes"]:
        label = shape["label"]
        if label in CLASS_MAPPING:
            points = np.array(shape["points"], dtype=np.float32)
            points[:, 0] *= IMG_SIZE[0] / data["imageWidth"]
            points[:, 1] *= IMG_SIZE[1] / data["imageHeight"]
            points = points.astype(np.int32)
            cv2.fillPoly(mask, [points], CLASS_MAPPING[label])

    # ğŸ¯ One-hot encoding
    mask = to_categorical(mask, num_classes=NUM_CLASSES)

    print(f"âœ… JSONì—ì„œ ë¶ˆëŸ¬ì˜¨ ì´ë¯¸ì§€ ê²½ë¡œ: {data['imagePath']}")
    print(f"âœ… ìµœì¢… ì´ë¯¸ì§€ ê²½ë¡œ: {image_path}")
    print(f"âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(image_path)}")

    return image / 255.0, mask


# ğŸ”¹ ëª¨ë“  JSON íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
json_files = glob.glob(os.path.join(JSON_DIR, "*.json"))

# ğŸ”¹ TensorFlow Dataset ìƒì„±
def data_generator():
    for json_file in json_files:
        image, mask = load_data(json_file)
        if image is not None and mask is not None:
            yield image, mask


dataset = tf.data.Dataset.from_generator(
    data_generator,
    output_signature=(
        tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(256, 256, NUM_CLASSES), dtype=tf.float32),
    )
)

dataset = dataset.shuffle(100).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# ğŸ”¹ U-Net ëª¨ë¸ ì •ì˜
def unet_model(input_size=(256, 256, 3), num_classes=NUM_CLASSES):
    inputs = layers.Input(input_size)

    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)
    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)
    p1 = layers.MaxPooling2D()(c1)

    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)
    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)
    p2 = layers.MaxPooling2D()(c2)

    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)
    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)
    p3 = layers.MaxPooling2D()(c3)

    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)
    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(c4)
    p4 = layers.MaxPooling2D()(c4)

    bn = layers.Conv2D(1024, 3, activation='relu', padding='same')(p4)
    bn = layers.Conv2D(1024, 3, activation='relu', padding='same')(bn)

    u1 = layers.UpSampling2D()(bn)
    u1 = layers.concatenate([u1, c4])
    c5 = layers.Conv2D(512, 3, activation='relu', padding='same')(u1)

    u2 = layers.UpSampling2D()(c5)
    u2 = layers.concatenate([u2, c3])
    c6 = layers.Conv2D(256, 3, activation='relu', padding='same')(u2)

    u3 = layers.UpSampling2D()(c6)
    u3 = layers.concatenate([u3, c2])
    c7 = layers.Conv2D(128, 3, activation='relu', padding='same')(u3)

    u4 = layers.UpSampling2D()(c7)
    u4 = layers.concatenate([u4, c1])
    c8 = layers.Conv2D(64, 3, activation='relu', padding='same')(u4)

    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(c8)

    return models.Model(inputs, outputs)

# ğŸ”¹ ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
model = unet_model()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ğŸ”¹ ëª¨ë¸ í•™ìŠµ
history = model.fit(dataset, epochs=EPOCHS)

# ğŸ”¹ ëª¨ë¸ ì €ì¥
model.save(os.path.join(OUTPUT_DIR, "unet_sea_segmentation.h5"))

# ğŸ”¹ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥
for json_file in json_files[:5]:  # ìƒ˜í”Œ 5ê°œë§Œ ì €ì¥
    image, _ = load_data(json_file)
    prediction = model.predict(np.expand_dims(image, axis=0))[0]

    pred_mask = np.argmax(prediction, axis=-1)

    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(image)
    plt.title("Original Image")

    plt.subplot(1, 2, 2)
    plt.imshow(pred_mask, cmap="jet")
    plt.title("Predicted Mask")

    output_file = os.path.join(OUTPUT_DIR, os.path.basename(json_file).replace(".json", "_pred.jpg"))
    plt.savefig(output_file)
    plt.close()